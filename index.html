import React, { useState } from 'react';
import { ChevronRight, ChevronLeft, RotateCcw, BookOpen, TrendingUp, Code, Calculator } from 'lucide-react';

const InterviewPrep = () => {
  const [activeCategory, setActiveCategory] = useState('ml-theory');
  const [currentCard, setCurrentCard] = useState(0);
  const [showAnswer, setShowAnswer] = useState(false);
  const [completedCards, setCompletedCards] = useState(new Set());

  const questions = {
    'ml-theory': [
      {
        q: "What is the bias-variance tradeoff?",
        a: "The bias-variance tradeoff describes the relationship between model complexity and prediction error. High bias (underfitting) means the model is too simple and makes strong assumptions. High variance (overfitting) means the model is too complex and captures noise. The goal is to find the sweet spot that minimizes total error = bias² + variance + irreducible error."
      },
      {
        q: "Explain the difference between L1 and L2 regularization.",
        a: "L1 (Lasso) adds the absolute value of coefficients as penalty (λ∑|w|), promoting sparsity and feature selection. L2 (Ridge) adds the square of coefficients (λ∑w²), shrinking all coefficients but rarely to zero. L1 is better for feature selection; L2 is better when all features are relevant."
      },
      {
        q: "What is gradient descent and why might it fail?",
        a: "Gradient descent iteratively updates parameters in the direction of steepest descent: w = w - α∇L(w). It can fail due to: 1) Getting stuck in local minima (less common in high dimensions), 2) Poor learning rate (too high causes divergence, too low causes slow convergence), 3) Saddle points in high dimensions, 4) Vanishing/exploding gradients."
      },
      {
        q: "Explain precision vs recall with an example.",
        a: "Precision = TP/(TP+FP) - of predicted positives, how many are correct. Recall = TP/(TP+FN) - of actual positives, how many did we find. Example: Email spam filter. High precision = few false alarms (good emails marked spam). High recall = catches most spam (but might flag good emails too). F1-score balances both."
      },
      {
        q: "What is cross-validation and why use it?",
        a: "Cross-validation splits data into k folds, trains on k-1 and validates on 1, rotating through all folds. Benefits: 1) Better estimate of model performance on unseen data, 2) Uses all data for both training and validation, 3) Reduces variance in performance estimates, 4) Helps detect overfitting. K-fold (k=5 or 10) is most common."
      },
      {
        q: "Explain the curse of dimensionality.",
        a: "As dimensions increase: 1) Data becomes sparse (exponentially more space to cover), 2) Distance metrics become less meaningful (all points equidistant), 3) Need exponentially more data, 4) Overfitting risk increases. Solutions include: dimensionality reduction (PCA, t-SNE), feature selection, and regularization."
      },
      {
        q: "What are eigenvectors and eigenvalues? Why are they important in ML?",
        a: "For matrix A, eigenvector v satisfies Av = λv where λ is eigenvalue. Direction doesn't change, only scaled. ML uses: 1) PCA (eigenvectors of covariance matrix are principal components), 2) Spectral clustering, 3) PageRank, 4) Understanding neural network dynamics, 5) Matrix decomposition."
      },
      {
        q: "Explain batch normalization and why it helps.",
        a: "Batch norm normalizes layer inputs to mean=0, std=1 across mini-batch, then scales/shifts with learned parameters. Benefits: 1) Allows higher learning rates, 2) Reduces internal covariate shift, 3) Acts as regularization, 4) Makes network less sensitive to initialization. Applied before or after activation function."
      }
    ],
    'statistics': [
      {
        q: "What is the Central Limit Theorem and why is it important?",
        a: "CLT states that the sampling distribution of the mean approaches normal distribution as sample size increases, regardless of population distribution. Importance: 1) Justifies using normal distribution in hypothesis testing, 2) Foundation for confidence intervals, 3) Enables statistical inference with large samples, 4) Works even with non-normal data."
      },
      {
        q: "Explain p-value and common misconceptions.",
        a: "P-value is P(observing data as extreme | H₀ is true). NOT the probability H₀ is true, or that results are due to chance. Common misconceptions: 1) p<0.05 doesn't mean result is important, 2) p>0.05 doesn't prove null hypothesis, 3) Smaller p-value doesn't mean larger effect, 4) P-hacking and multiple testing issues."
      },
      {
        q: "What is Maximum Likelihood Estimation (MLE)?",
        a: "MLE finds parameters that maximize the likelihood of observing the data: θ̂ = argmax L(θ|data) = argmax ∏P(xᵢ|θ). Often maximize log-likelihood for numerical stability. Properties: 1) Asymptotically unbiased, 2) Consistent, 3) Asymptotically normal, 4) Asymptotically efficient. Used in logistic regression, neural networks, etc."
      },
      {
        q: "Explain Bayes' Theorem with a practical example.",
        a: "P(A|B) = P(B|A)P(A)/P(B). Example: Disease testing. P(disease|positive) = P(positive|disease) × P(disease) / P(positive). If disease rate is 0.1%, test sensitivity 99%, specificity 95%, then P(disease|positive) ≈ 2%, not 99%! Base rate matters hugely. Used in: naive Bayes, Bayesian inference, spam filtering."
      },
      {
        q: "What is the difference between correlation and causation?",
        a: "Correlation measures statistical association (r ∈ [-1,1]). Causation means X causes Y. Correlation ≠ causation because: 1) Reverse causality, 2) Confounding variables, 3) Spurious correlations. To establish causation need: 1) Randomized controlled trials, 2) Natural experiments, 3) Causal inference methods (instrumental variables, difference-in-differences, etc.)"
      },
      {
        q: "Explain Type I and Type II errors with consequences.",
        a: "Type I (α): False positive - reject true H₀. Type II (β): False negative - fail to reject false H₀. Power = 1-β. Trade-off depends on consequences. Medical test: Type I = healthy person treated unnecessarily. Type II = sick person untreated (often worse). Set α based on risk tolerance (typically 0.05)."
      }
    ],
    'coding': [
      {
        q: "Implement k-nearest neighbors from scratch (pseudocode).",
        a: "```python\ndef knn_predict(X_train, y_train, x_test, k):\n  # Calculate distances\n  distances = [euclidean_distance(x_test, x) for x in X_train]\n  # Get k nearest indices\n  k_indices = argsort(distances)[:k]\n  # Get k nearest labels\n  k_nearest_labels = [y_train[i] for i in k_indices]\n  # Return majority vote (classification) or mean (regression)\n  return mode(k_nearest_labels)  # or mean()\n```\nTime: O(n) per prediction. Space: O(n). Can optimize with KD-trees for low dimensions."
      },
      {
        q: "Write gradient descent for linear regression.",
        a: "```python\ndef gradient_descent(X, y, lr=0.01, epochs=1000):\n  m, n = X.shape\n  w = np.zeros(n)\n  b = 0\n  for _ in range(epochs):\n    y_pred = X @ w + b\n    # Gradients\n    dw = (1/m) * X.T @ (y_pred - y)\n    db = (1/m) * np.sum(y_pred - y)\n    # Update\n    w -= lr * dw\n    b -= lr * db\n  return w, b\n```\nLoss: MSE = (1/2m)∑(ŷ-y)². Derivative: ∂L/∂w = (1/m)Xᵀ(ŷ-y)"
      },
      {
        q: "Implement sigmoid and its derivative.",
        a: "```python\ndef sigmoid(x):\n  return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n  s = sigmoid(x)\n  return s * (1 - s)\n```\nUseful property: σ'(x) = σ(x)(1-σ(x)) makes backprop efficient. Range: (0,1). Used in logistic regression and neural network activations. Note: Can cause vanishing gradients for large |x|."
      },
      {
        q: "How would you implement train/validation/test split?",
        a: "```python\nfrom sklearn.model_selection import train_test_split\n\n# First split: separate test set (80/20)\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Second split: separate train/val (75/25 of temp = 60/20 overall)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n```\nRatios: 60% train, 20% val, 20% test. Use stratify for balanced splits."
      },
      {
        q: "Implement softmax function numerically stable.",
        a: "```python\ndef softmax(x):\n  # Subtract max for numerical stability\n  exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n  return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n```\nWithout max subtraction, exp(large number) causes overflow. This trick doesn't change result since softmax(x+c) = softmax(x). Returns probabilities summing to 1. Used in multi-class classification."
      }
    ],
    'quant': [
      {
        q: "Expected value of the maximum of two dice rolls?",
        a: "Let X, Y be two dice. E[max(X,Y)] = ∑∑ max(i,j)P(X=i,Y=j). Since independent: P(X=i,Y=j) = 1/36. Can compute: E[max] = 4.472 (≈ 161/36). Alternative: E[max] = E[X] + E[Y] - E[min]. Or use: P(max≤k) = P(X≤k)P(Y≤k) = (k/6)², then sum. Answer ≈ 4.47."
      },
      {
        q: "You have 100 coins, one is double-headed. You pick randomly, flip 10 times, get 10 heads. Probability it's the double-headed coin?",
        a: "Use Bayes: P(double|10H) = P(10H|double)P(double) / P(10H). P(double) = 1/100. P(10H|double) = 1. P(10H|normal) = (1/2)¹⁰ = 1/1024. P(10H) = (1/100)(1) + (99/100)(1/1024) = 1/100 + 99/102400. P(double|10H) = (1/100) / (1/100 + 99/102400) ≈ 0.912 or 91.2%."
      },
      {
        q: "Expected number of coin flips to get two heads in a row?",
        a: "Let E = expected flips from start. After first flip: if T (prob 1/2), start over = 1 + E. If H (prob 1/2), need one more: if T (prob 1/2), waste 2 flips, start over = 2 + E. If H (prob 1/2), done = 2. E = 1 + (1/2)E + (1/2)[(1/2)(2+E) + (1/2)(2)]. E = 1 + E/2 + (1/2)[1 + E/2 + 1]. E = 6. Answer: 6 flips."
      },
      {
        q: "You're given a random number from uniform[0,1]. You can accept it or reject and draw again. What strategy maximizes your expected value?",
        a: "Optimal stopping problem. Strategy: Set threshold T. Accept if x ≥ T, reject if x < T. E[value|accept] = (1+T)/2. P(accept immediately) = 1-T. P(reject) = T, then get new draw. E[total] = (1-T)(1+T)/2 + T·E[total]. Solve: E = (1-T²)/2/(1-T) = (1+T)/2. Maximized at T = 0, but that's trivial. With limited draws, use secretary problem logic. With infinite draws and no discounting, accept first draw."
      },
      {
        q: "Monty Hall problem: Should you switch doors?",
        a: "Yes, switch. Initial choice: P(car) = 1/3, P(goat) = 2/3. After Monty reveals goat: If you picked car initially (1/3), switching loses. If you picked goat initially (2/3), switching wins. So P(win|switch) = 2/3, P(win|stay) = 1/3. Switch doubles your chances. Bayes theorem confirms this counterintuitive result."
      },
      {
        q: "Expected time until first head if coin is flipped every minute?",
        a: "Geometric distribution: E[X] = 1/p where p = 0.5. E[flips] = 2. But note: each flip takes 1 minute, so 2 flips = 2 minutes expected? Actually, if we start timing at flip 1, then first success at flip n means n minutes. E[time] = E[flips] = 2 minutes. Variance = (1-p)/p² = 2 minutes²."
      },
      {
        q: "What is the Sharpe Ratio and why is it important?",
        a: "Sharpe Ratio = (E[R] - Rf) / σ(R), where R is return, Rf is risk-free rate, σ is standard deviation. Measures risk-adjusted return: excess return per unit of volatility. Higher is better. Typical values: <1 (bad), 1-2 (good), >2 (excellent). Issues: 1) Assumes normal distribution, 2) Uses std dev (penalizes upside volatility), 3) Can be gamed. Alternatives: Sortino (downside deviation), Calmar (max drawdown)."
      },
      {
        q: "Explain the Black-Scholes formula intuition.",
        a: "Call option: C = S₀N(d₁) - Ke^(-rT)N(d₂). Intuition: Expected value in risk-neutral world. S₀N(d₁) = expected stock value if exercised (delta-hedged). Ke^(-rT)N(d₂) = present value of strike, weighted by prob of exercise. N(d₂) ≈ risk-neutral prob of exercise. Assumes: log-normal prices, constant volatility, no dividends, continuous trading. Key insight: dynamic hedging eliminates risk."
      }
    ]
  };

  const categories = [
    { id: 'ml-theory', name: 'ML Theory', icon: BookOpen, color: 'bg-blue-500' },
    { id: 'statistics', name: 'Statistics', icon: TrendingUp, color: 'bg-purple-500' },
    { id: 'coding', name: 'Coding', icon: Code, color: 'bg-green-500' },
    { id: 'quant', name: 'Quant/Probability', icon: Calculator, color: 'bg-orange-500' }
  ];

  const currentQuestions = questions[activeCategory];
  const progress = (completedCards.size / currentQuestions.length) * 100;

  const nextCard = () => {
    if (currentCard < currentQuestions.length - 1) {
      setCurrentCard(currentCard + 1);
      setShowAnswer(false);
    }
  };

  const prevCard = () => {
    if (currentCard > 0) {
      setCurrentCard(currentCard - 1);
      setShowAnswer(false);
    }
  };

  const markComplete = () => {
    const key = `${activeCategory}-${currentCard}`;
    setCompletedCards(new Set([...completedCards, key]));
    if (currentCard < currentQuestions.length - 1) {
      nextCard();
    }
  };

  const resetProgress = () => {
    setCompletedCards(new Set());
    setCurrentCard(0);
    setShowAnswer(false);
  };

  const changeCategory = (catId) => {
    setActiveCategory(catId);
    setCurrentCard(0);
    setShowAnswer(false);
  };

  const isCurrentCardComplete = completedCards.has(`${activeCategory}-${currentCard}`);

  return (
    <div className="min-h-screen bg-gradient-to-br from-slate-900 via-slate-800 to-slate-900 text-white p-8">
      <div className="max-w-6xl mx-auto">
        {/* Header */}
        <div className="text-center mb-12">
          <h1 className="text-5xl font-bold mb-4 bg-gradient-to-r from-blue-400 to-purple-500 bg-clip-text text-transparent">
            ML & Quant Interview Prep
          </h1>
          <p className="text-slate-300 text-lg">Master the concepts that matter</p>
        </div>

        {/* Category Selection */}
        <div className="grid grid-cols-2 md:grid-cols-4 gap-4 mb-8">
          {categories.map(cat => {
            const Icon = cat.icon;
            const isActive = activeCategory === cat.id;
            const categoryCompleted = Array.from(completedCards).filter(k => k.startsWith(cat.id)).length;
            const categoryTotal = questions[cat.id].length;
            
            return (
              <button
                key={cat.id}
                onClick={() => changeCategory(cat.id)}
                className={`p-6 rounded-xl transition-all transform hover:scale-105 ${
                  isActive 
                    ? `${cat.color} shadow-lg shadow-${cat.color}/50` 
                    : 'bg-slate-800 hover:bg-slate-700'
                }`}
              >
                <Icon className="w-8 h-8 mx-auto mb-2" />
                <div className="font-semibold mb-1">{cat.name}</div>
                <div className="text-sm text-slate-300">{categoryCompleted}/{categoryTotal}</div>
              </button>
            );
          })}
        </div>

        {/* Progress Bar */}
        <div className="mb-8">
          <div className="flex justify-between text-sm text-slate-400 mb-2">
            <span>Progress: {Array.from(completedCards).filter(k => k.startsWith(activeCategory)).length}/{currentQuestions.length} completed</span>
            <button 
              onClick={resetProgress}
              className="flex items-center gap-1 hover:text-white transition-colors"
            >
              <RotateCcw className="w-4 h-4" />
              Reset
            </button>
          </div>
          <div className="w-full bg-slate-700 rounded-full h-3">
            <div 
              className="bg-gradient-to-r from-blue-500 to-purple-500 h-3 rounded-full transition-all duration-500"
              style={{ width: `${(Array.from(completedCards).filter(k => k.startsWith(activeCategory)).length / currentQuestions.length) * 100}%` }}
            />
          </div>
        </div>

        {/* Flashcard */}
        <div className="bg-slate-800 rounded-2xl shadow-2xl p-8 mb-6 min-h-96">
          <div className="flex justify-between items-center mb-6">
            <span className="text-slate-400">
              Question {currentCard + 1} of {currentQuestions.length}
            </span>
            {isCurrentCardComplete && (
              <span className="bg-green-500 text-white px-3 py-1 rounded-full text-sm">
                ✓ Completed
              </span>
            )}
          </div>

          <div className="space-y-6">
            <div>
              <h3 className="text-sm uppercase tracking-wide text-slate-400 mb-2">Question</h3>
              <p className="text-2xl font-medium leading-relaxed">
                {currentQuestions[currentCard].q}
              </p>
            </div>

            {showAnswer && (
              <div className="border-t border-slate-700 pt-6">
                <h3 className="text-sm uppercase tracking-wide text-slate-400 mb-2">Answer</h3>
                <p className="text-lg text-slate-200 leading-relaxed whitespace-pre-line">
                  {currentQuestions[currentCard].a}
                </p>
              </div>
            )}
          </div>

          <div className="flex gap-4 mt-8">
            {!showAnswer ? (
              <button
                onClick={() => setShowAnswer(true)}
                className="flex-1 bg-gradient-to-r from-blue-500 to-purple-500 hover:from-blue-600 hover:to-purple-600 py-4 rounded-xl font-semibold transition-all transform hover:scale-105"
              >
                Show Answer
              </button>
            ) : (
              <button
                onClick={markComplete}
                className="flex-1 bg-gradient-to-r from-green-500 to-emerald-500 hover:from-green-600 hover:to-emerald-600 py-4 rounded-xl font-semibold transition-all transform hover:scale-105"
              >
                Mark as Complete
              </button>
            )}
          </div>
        </div>

        {/* Navigation */}
        <div className="flex justify-between items-center">
          <button
            onClick={prevCard}
            disabled={currentCard === 0}
            className={`flex items-center gap-2 px-6 py-3 rounded-xl font-semibold transition-all ${
              currentCard === 0
                ? 'bg-slate-700 text-slate-500 cursor-not-allowed'
                : 'bg-slate-700 hover:bg-slate-600'
            }`}
          >
            <ChevronLeft className="w-5 h-5" />
            Previous
          </button>

          <button
            onClick={nextCard}
            disabled={currentCard === currentQuestions.length - 1}
            className={`flex items-center gap-2 px-6 py-3 rounded-xl font-semibold transition-all ${
              currentCard === currentQuestions.length - 1
                ? 'bg-slate-700 text-slate-500 cursor-not-allowed'
                : 'bg-slate-700 hover:bg-slate-600'
            }`}
          >
            Next
            <ChevronRight className="w-5 h-5" />
          </button>
        </div>

        {/* Tips */}
        <div className="mt-12 bg-slate-800 rounded-xl p-6 border border-slate-700">
          <h3 className="font-semibold text-lg mb-3">💡 Study Tips</h3>
          <ul className="space-y-2 text-slate-300">
            <li>• Try to answer each question out loud before revealing the answer</li>
            <li>• Focus on understanding the intuition, not just memorizing formulas</li>
            <li>• For coding problems, practice implementing them from scratch</li>
            <li>• Review completed cards periodically for spaced repetition</li>
          </ul>
        </div>
      </div>
    </div>
  );
};

export default InterviewPrep;
